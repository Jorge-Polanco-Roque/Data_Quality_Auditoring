---
name: data-quality-auditor
description: >
  Framework experto para construir un auditor de calidad de datos dinÃ¡mico en Python.
  Recibe cualquier CSV, detecta automÃ¡ticamente tipos de dato, ejecuta pruebas
  estadÃ­sticas y heurÃ­sticas por tipo, y genera un reporte estandarizado con
  severidad de issues. Usar cuando se necesite: (1) Construir un script de calidad
  de datos reutilizable, (2) Detectar anomalÃ­as, outliers, nulls y errores de formato,
  (3) Identificar cambios de tendencia y drift estadÃ­stico, (4) Generar reportes
  ejecutivos con puntos crÃ­ticos clasificados por severidad.
---

# Data Quality Auditor â€” Skill de ConstrucciÃ³n

Framework experto para construir `data_quality_auditor.py`, un sistema dinÃ¡mico de
auditorÃ­a de calidad que funciona sobre cualquier CSV sin configuraciÃ³n previa.

---

## Arquitectura del Sistema

El script se divide en **6 capas independientes** que se ejecutan en cadena:

```
CSV Input
   â”‚
   â–¼
[1] DataLoader          â† Carga, encoding, delimiters
   â”‚
   â–¼
[2] TypeDetector        â† Infiere tipo semÃ¡ntico de cada columna
   â”‚
   â–¼
[3] CheckRegistry       â† Mapa de checks por tipo de dato
   â”‚
   â–¼
[4] CheckEngine         â† Ejecuta checks, captura resultados
   â”‚
   â–¼
[5] ScoringSystem       â† Asigna severidad (CRITICAL/HIGH/MEDIUM/LOW/INFO)
   â”‚
   â–¼
[6] ReportBuilder       â† Genera reporte estandarizado JSON + texto
```

---

## Capa 1 â€” DataLoader

**Responsabilidad:** Cargar el CSV de forma robusta sin asumir nada del archivo.

**LÃ³gica de implementaciÃ³n:**
- Detectar encoding con `chardet` antes de leer con pandas
- Intentar delimiters en orden: `,` â†’ `;` â†’ `\t` â†’ `|`; usar el que produzca mÃ¡s columnas
- Leer todo como `dtype=str` primero (preservar datos crudos para inspecciÃ³n de formato)
- Generar `df_raw` (strings puros) y `df` (con tipos inferidos por pandas) en paralelo
- Registrar metadata: `n_rows`, `n_cols`, `file_size_mb`, `encoding`, `delimiter`

**Output:** `(df_raw, df, metadata_dict)`

---

## Capa 2 â€” TypeDetector

**Responsabilidad:** Asignar un **tipo semÃ¡ntico** a cada columna, mÃ¡s allÃ¡ del dtype de pandas.

### Tipos SemÃ¡nticos Soportados

| Tipo SemÃ¡ntico     | DescripciÃ³n                                              |
|--------------------|----------------------------------------------------------|
| `NUMERIC_CONTINUOUS` | Floats o ints con alta cardinalidad (precios, mÃ©tricas) |
| `NUMERIC_DISCRETE`   | Ints con baja cardinalidad (conteos, ratings, edades)   |
| `CATEGORICAL`        | Strings con cardinalidad baja relativa a n_rows         |
| `HIGH_CARDINALITY`   | Strings con cardinalidad >50% de n_rows                 |
| `BOOLEAN`            | Columnas con 2 valores Ãºnicos (true/false, 0/1, si/no)  |
| `DATE`               | Columnas parseables como fecha                          |
| `DATETIME`           | Columnas parseables como fecha+hora                     |
| `EMAIL`              | Strings que coinciden con patrÃ³n de email               |
| `PHONE`              | Strings que coinciden con patrones telefÃ³nicos          |
| `ID_CANDIDATE`       | Alta unicidad + patrÃ³n estructurado (UUID, cÃ³digo)      |
| `MIXED`              | Columna con mezcla de tipos detectados                  |
| `EMPTY`              | >95% valores nulos o vacÃ­os                             |
| `CONSTANT`           | Un solo valor Ãºnico en toda la columna                  |

### Algoritmo de DetecciÃ³n

Para cada columna, ejecutar en este orden (el primero que aplique gana):

```
1. Si null_pct >= 0.95  â†’ EMPTY
2. Si n_unique == 1     â†’ CONSTANT
3. Si n_unique == 2 y valores son variantes de true/false/0/1/si/no â†’ BOOLEAN
4. Si pandas dtype es numeric:
     Si n_unique / n_rows_nonnull < 0.05 â†’ NUMERIC_DISCRETE
     Else â†’ NUMERIC_CONTINUOUS
5. Si pandas dtype es object:
     a. Intentar parsear muestra de 200 valores como fecha â†’ si >80% parsea â†’ DATE o DATETIME
     b. Regex email sobre muestra â†’ si >80% â†’ EMAIL
     c. Regex phone sobre muestra â†’ si >80% â†’ PHONE
     d. Si n_unique / n_rows_nonnull > 0.85 â†’ ID_CANDIDATE o HIGH_CARDINALITY
     e. Si n_unique / n_rows_nonnull < 0.15 â†’ CATEGORICAL
     f. Else â†’ HIGH_CARDINALITY
6. Si hay mezcla detectada en pasos anteriores â†’ MIXED
```

**Para fechas:** intentar mÃºltiples formatos: ISO 8601, `dd/mm/yyyy`, `mm/dd/yyyy`,
`yyyy-mm-dd HH:MM:SS`, `dd-Mon-yyyy`, Unix timestamp numÃ©rico.
Registrar el formato dominante y los formatos alternativos encontrados.

**Output:** `column_type_map: Dict[str, SemanticType]`

---

## Capa 3 â€” CheckRegistry

**Responsabilidad:** Mapa declarativo de quÃ© checks aplican a cada tipo semÃ¡ntico.

### Estructura del Registro

Cada check es un objeto con:
- `check_id`: identificador Ãºnico (`"NULL_RATE"`, `"OUTLIER_IQR"`, etc.)
- `applies_to`: lista de tipos semÃ¡nticos donde corre
- `function`: callable que recibe `(series_raw, series_typed, metadata)` y retorna `CheckResult`
- `severity_rules`: dict de umbrales â†’ severidad

### Mapa de Checks por Tipo

#### Checks Universales (todos los tipos)
| Check ID              | DescripciÃ³n                                          |
|-----------------------|------------------------------------------------------|
| `NULL_RATE`           | % de nulos/NaN/strings vacÃ­os                        |
| `DUPLICATE_ROWS`      | Filas completamente duplicadas (solo en nivel global)|
| `WHITESPACE_ISSUES`   | Valores con espacios leading/trailing                |
| `CONSTANT_COLUMN`     | Columna con un solo valor Ãºnico                      |
| `NEAR_CONSTANT`       | Un valor representa >95% de los registros            |

#### Checks NumÃ©ricos (`NUMERIC_CONTINUOUS`, `NUMERIC_DISCRETE`)
| Check ID              | DescripciÃ³n                                          |
|-----------------------|------------------------------------------------------|
| `OUTLIER_IQR`         | Valores fuera de 1.5Ã—IQR (Tukey)                    |
| `OUTLIER_ZSCORE`      | Valores con \|z-score\| > 3                          |
| `OUTLIER_MODIFIED_Z`  | Modified Z-score con MAD para distribuciones sesgadas|
| `DISTRIBUTION_SKEW`   | Skewness > 2 o < -2 (distribuciÃ³n muy asimÃ©trica)   |
| `DISTRIBUTION_KURTOSIS` | Kurtosis excesiva (colas pesadas)                  |
| `NEGATIVE_VALUES`     | Presencia de negativos en columnas que no deberÃ­an   |
| `ZERO_VALUES`         | % de ceros (puede indicar valores faltantes codificados)|
| `TREND_CHANGE`        | Cambio significativo en media mÃ³vil vs histÃ³rico     |
| `VALUE_RANGE`         | Valores fuera del rango percentil [0.1, 99.9]        |
| `VARIANCE_SUDDEN_CHANGE` | Cambio abrupto en varianza entre segmentos del df |
| `NORMALITY_TEST`      | Shapiro-Wilk (n<5000) o D'Agostino-KÂ² para normalidad|

#### Checks de Fechas (`DATE`, `DATETIME`)
| Check ID              | DescripciÃ³n                                          |
|-----------------------|------------------------------------------------------|
| `DATE_NULL_RATE`      | Nulos en columna de fecha (crÃ­tico para series temporales)|
| `DATE_FORMAT_MIX`     | MÃºltiples formatos de fecha en la misma columna      |
| `DATE_FUTURE`         | Fechas futuras (si no se esperan)                    |
| `DATE_ANCIENT`        | Fechas antes de 1900 (posible error de dato)         |
| `DATE_SEQUENCE_GAPS`  | Gaps inesperados en series temporales               |
| `DATE_DUPLICATES`     | Fechas duplicadas (si se espera unicidad)            |
| `DATE_MONOTONICITY`   | Verifica que la columna estÃ© ordenada si deberÃ­a     |
| `DATE_INVALID_PARSED` | Valores que no pudieron parsearse como fecha vÃ¡lida  |
| `TEMPORAL_DRIFT`      | Cambio en distribuciÃ³n de valores a lo largo del tiempo |

#### Checks CategÃ³ricos (`CATEGORICAL`, `BOOLEAN`)
| Check ID              | DescripciÃ³n                                          |
|-----------------------|------------------------------------------------------|
| `RARE_CATEGORIES`     | CategorÃ­as con frecuencia < 0.5% del total           |
| `CARDINALITY_CHANGE`  | Nuevas categorÃ­as vs las esperadas (si hay referencia)|
| `CASE_INCONSISTENCY`  | Misma categorÃ­a con diferente capitalizaciÃ³n         |
| `ENCODING_ANOMALY`    | Caracteres raros o de control en categorÃ­as          |
| `CLASS_IMBALANCE`     | Una categorÃ­a representa >95% de los datos           |
| `TYPO_CANDIDATES`     | CategorÃ­as similares por distancia de Levenshtein    |

#### Checks de Texto (`HIGH_CARDINALITY`, `EMAIL`, `PHONE`)
| Check ID              | DescripciÃ³n                                          |
|-----------------------|------------------------------------------------------|
| `EMAIL_FORMAT`        | Emails que no cumplen RFC 5322 bÃ¡sico                |
| `PHONE_FORMAT`        | TelÃ©fonos que no cumplen patrÃ³n esperado (E.164 o local)|
| `LENGTH_OUTLIERS`     | Longitud de string muy fuera del rango tÃ­pico        |
| `NULL_LIKE_STRINGS`   | Strings que son "N/A", "null", "none", "NA", "NaN", "-"|
| `TRUNCATION_SIGNS`    | Valores que terminan abruptamente (posible truncaciÃ³n)|

#### Checks de IDs (`ID_CANDIDATE`)
| Check ID              | DescripciÃ³n                                          |
|-----------------------|------------------------------------------------------|
| `ID_DUPLICATES`       | IDs duplicados (generalmente crÃ­tico)                |
| `ID_FORMAT_CONSISTENCY` | PatrÃ³n de formato inconsistente en IDs             |
| `ID_NULL`             | Nulos en columna de ID                               |

---

## Capa 4 â€” CheckEngine

**Responsabilidad:** Ejecutar todos los checks aplicables y capturar resultados de forma segura.

### CheckResult Schema

```python
@dataclass
class CheckResult:
    check_id: str
    column: str
    passed: bool
    severity: str        # CRITICAL | HIGH | MEDIUM | LOW | INFO | PASS
    value: float         # valor medido (ej: 0.23 para 23% de nulls)
    threshold: float     # umbral que se violÃ³
    message: str         # descripciÃ³n legible
    affected_count: int  # nÂ° de registros afectados
    affected_pct: float  # % de registros afectados
    sample_values: list  # hasta 5 ejemplos de valores problemÃ¡ticos
    metadata: dict       # datos adicionales del check
```

### ImplementaciÃ³n de Checks Clave

#### NULL_RATE
```python
null_pct = (series.isna() | (series.astype(str).str.strip() == '')).mean()
thresholds = {0.5: 'CRITICAL', 0.2: 'HIGH', 0.05: 'MEDIUM', 0.01: 'LOW'}
```

#### OUTLIER_IQR
```python
Q1, Q3 = series.quantile(0.25), series.quantile(0.75)
IQR = Q3 - Q1
mask = (series < Q1 - 1.5*IQR) | (series > Q3 + 1.5*IQR)
# Severidad basada en outlier_pct y magnitud de desviaciÃ³n
```

#### OUTLIER_MODIFIED_Z (robusto para distribuciones no normales)
```python
median = series.median()
MAD = (series - median).abs().median()
modified_z = 0.6745 * (series - median) / MAD
mask = modified_z.abs() > 3.5
```

#### TREND_CHANGE (detecciÃ³n de drift)
```python
# Dividir serie en N ventanas temporales iguales
# Calcular media de cada ventana
# Comparar cada ventana vs media global: si |delta| > 2*std_global â†’ alerta
# TambiÃ©n: Mann-Kendall test para tendencia monotÃ³nica
```

#### DATE_FORMAT_MIX
```python
# Para cada valor no-nulo, intentar parsear con N formatos conocidos
# Registrar quÃ© formato parseÃ³ cada valor
# Si hay mÃ¡s de 1 formato activo â†’ HIGH
# Si hay >3 formatos activos â†’ CRITICAL
```

#### TYPO_CANDIDATES (categorÃ­as similares)
```python
from rapidfuzz import fuzz
# Para cada par de categorÃ­as Ãºnicas con frecuencia > 1
# Si Levenshtein similarity > 85% y son diferentes â†’ candidatos a typo
# Reportar pares sospechosos
```

#### TEMPORAL_DRIFT
```python
# Si existe columna de fecha, ordenar df por ella
# Dividir en cuartiles temporales
# Para cada columna numÃ©rica: comparar distribuciÃ³n en Q1 vs Q4
# Usar KS test (scipy.stats.ks_2samp): si p < 0.05 â†’ drift significativo
```

### Manejo de Errores en Engine
- Cada check corre en `try/except`; si falla, genera `CheckResult` con `severity='INFO'` y mensaje de error
- No detener el anÃ¡lisis completo por fallo de un check individual
- Loggear warnings internos sin romper el flujo

---

## Capa 5 â€” ScoringSystem

**Responsabilidad:** Agregar resultados y calcular score de salud por columna y global.

### Severity Levels

| Nivel      | DescripciÃ³n                                           | AcciÃ³n Recomendada                    |
|------------|-------------------------------------------------------|---------------------------------------|
| `CRITICAL` | Problema grave que compromete la usabilidad del dato  | Detener pipeline, investigar de inmediato |
| `HIGH`     | Problema significativo que afecta anÃ¡lisis             | Resolver antes de cualquier uso        |
| `MEDIUM`   | Problema moderado, puede sesgar resultados             | Documentar y evaluar impacto           |
| `LOW`      | AnomalÃ­a menor, probablemente aceptable                | Registrar y monitorear                 |
| `INFO`     | ObservaciÃ³n sin impacto directo                        | Opcional revisiÃ³n                      |
| `PASS`     | Check superado sin problemas                           | â€”                                      |

### Column Health Score

```
score = 100
- por cada CRITICAL: -25 puntos
- por cada HIGH:     -10 puntos
- por cada MEDIUM:   -5 puntos
- por cada LOW:      -2 puntos
score = max(0, score)

Grade: A (90-100) | B (75-89) | C (60-74) | D (40-59) | F (<40)
```

### Dataset Health Score
```
dataset_score = media ponderada de column scores
weight por columna = 1 / (1 + null_pct)  â† columnas mÃ¡s completas pesan mÃ¡s
```

---

## Capa 6 â€” ReportBuilder

**Responsabilidad:** Generar reporte estandarizado completo en mÃºltiples formatos.

### Estructura del Reporte JSON

```json
{
  "report_metadata": {
    "generated_at": "ISO timestamp",
    "file_analyzed": "nombre del archivo",
    "total_rows": 0,
    "total_columns": 0,
    "encoding": "utf-8",
    "delimiter": ","
  },
  "dataset_summary": {
    "health_score": 0.0,
    "health_grade": "A|B|C|D|F",
    "total_issues": 0,
    "issues_by_severity": {
      "CRITICAL": 0, "HIGH": 0, "MEDIUM": 0, "LOW": 0
    },
    "critical_columns": ["col1", "col2"],
    "clean_columns": ["col3"]
  },
  "column_profiles": {
    "column_name": {
      "semantic_type": "NUMERIC_CONTINUOUS",
      "pandas_dtype": "float64",
      "n_unique": 0,
      "null_pct": 0.0,
      "health_score": 0.0,
      "health_grade": "A",
      "checks_run": 0,
      "checks_failed": 0,
      "issues": []
    }
  },
  "critical_issues": [
    {
      "check_id": "NULL_RATE",
      "column": "col1",
      "severity": "CRITICAL",
      "message": "53% de valores nulos",
      "affected_count": 530,
      "affected_pct": 0.53,
      "sample_values": []
    }
  ],
  "recommendations": [
    {
      "priority": 1,
      "category": "Missing Data",
      "column": "col1",
      "action": "Investigar fuente de nulls; evaluar imputaciÃ³n o exclusiÃ³n",
      "estimated_impact": "HIGH"
    }
  ],
  "statistical_summary": {
    "numeric_columns": {
      "col_name": {
        "mean": 0.0, "median": 0.0, "std": 0.0,
        "min": 0.0, "max": 0.0,
        "skewness": 0.0, "kurtosis": 0.0,
        "outlier_count_iqr": 0, "outlier_count_zscore": 0
      }
    },
    "categorical_columns": {
      "col_name": {
        "n_unique": 0, "top_value": "", "top_freq": 0.0,
        "rare_categories": []
      }
    },
    "date_columns": {
      "col_name": {
        "min_date": "", "max_date": "",
        "formats_found": [], "gap_count": 0
      }
    }
  }
}
```

### Reporte de Texto (stdout / .txt)

El reporte de texto debe seguir esta plantilla estandarizada:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           DATA QUALITY AUDIT REPORT                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Archivo     : {filename}
Filas       : {n_rows:,}
Columnas    : {n_cols}
Generado    : {timestamp}
Health Score: {score}/100  ({grade})

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
RESUMEN DE ISSUES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  ğŸ”´ CRITICAL : {n_critical}
  ğŸŸ  HIGH     : {n_high}
  ğŸŸ¡ MEDIUM   : {n_medium}
  ğŸŸ¢ LOW      : {n_low}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
PUNTOS CRÃTICOS (requieren acciÃ³n inmediata)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{por cada issue CRITICAL o HIGH}
  [{severity}] {column} â†’ {check_id}
  Detalle   : {message}
  Afectados : {affected_count:,} registros ({affected_pct:.1%})
  Muestra   : {sample_values}
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
REPORTE POR COLUMNA
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{por cada columna}
  {col_name} [{semantic_type}] â€” Score: {score}/100 ({grade})
  Nulls: {null_pct:.1%} | Ãšnicos: {n_unique:,}
  Issues: {lista de issues con severidad}
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
RECOMENDACIONES PRIORIZADAS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{ordenadas por prioridad}
  #{n}. [{category}] {column}: {action}
```

---

## Estructura de Archivos del Proyecto

```
data_quality_auditor/
â”‚
â”œâ”€â”€ data_quality_auditor.py        â† Entry point principal (CLI)
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py             â† Capa 1
â”‚   â”œâ”€â”€ type_detector.py           â† Capa 2
â”‚   â”œâ”€â”€ check_registry.py          â† Capa 3 (definiciÃ³n declarativa)
â”‚   â”œâ”€â”€ check_engine.py            â† Capa 4
â”‚   â”œâ”€â”€ scoring_system.py          â† Capa 5
â”‚   â””â”€â”€ report_builder.py          â† Capa 6
â”‚
â”œâ”€â”€ checks/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ universal_checks.py        â† Checks para todos los tipos
â”‚   â”œâ”€â”€ numeric_checks.py          â† Checks numÃ©ricos
â”‚   â”œâ”€â”€ date_checks.py             â† Checks de fechas
â”‚   â”œâ”€â”€ categorical_checks.py      â† Checks categÃ³ricos
â”‚   â”œâ”€â”€ text_checks.py             â† Checks de texto/email/phone
â”‚   â””â”€â”€ id_checks.py               â† Checks de IDs
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ check_result.py            â† Dataclass CheckResult
â”‚   â””â”€â”€ semantic_type.py           â† Enum SemanticType
â”‚
â””â”€â”€ requirements.txt
```

---

## Requirements

```txt
pandas>=2.0.0
numpy>=1.24.0
scipy>=1.11.0
chardet>=5.0.0
rapidfuzz>=3.0.0       # Para detecciÃ³n de typos en categÃ³ricos
python-dateutil>=2.8.0 # Para parsing flexible de fechas
pymannkendall>=1.4.3   # Para Mann-Kendall trend test
rich>=13.0.0           # Para reporte en consola con colores
```

---

## CLI Interface

```bash
# Uso bÃ¡sico
python data_quality_auditor.py --input data.csv

# Con output de reporte
python data_quality_auditor.py --input data.csv --output report.json

# Solo mostrar issues CRITICAL y HIGH
python data_quality_auditor.py --input data.csv --min-severity HIGH

# Definir columna de fecha para anÃ¡lisis temporal
python data_quality_auditor.py --input data.csv --date-col fecha

# Exportar reporte de texto tambiÃ©n
python data_quality_auditor.py --input data.csv --output report.json --text-report report.txt

# Modo silencioso (solo exit code: 0=ok, 1=issues, 2=critical)
python data_quality_auditor.py --input data.csv --quiet
```

---

## GuÃ­a de ImplementaciÃ³n para Claude

Cuando el usuario pida construir este proyecto, seguir este orden:

**Paso 1 â€” Modelos base**
Crear `models/semantic_type.py` (Enum) y `models/check_result.py` (dataclass).
Estos no tienen dependencias y todo lo demÃ¡s los usa.

**Paso 2 â€” DataLoader**
Implementar `core/data_loader.py` con detecciÃ³n de encoding y delimiter.
Probar que carga correctamente antes de continuar.

**Paso 3 â€” TypeDetector**
Implementar `core/type_detector.py` siguiendo el algoritmo de detecciÃ³n en el orden
exacto descrito en Capa 2. Incluir todos los tipos semÃ¡nticos del enum.

**Paso 4 â€” Checks (empezar por universales y numÃ©ricos)**
Implementar checks en `checks/` uno por mÃ³dulo. Empezar con `universal_checks.py`
y `numeric_checks.py` que son los mÃ¡s usados.

**Paso 5 â€” CheckRegistry + CheckEngine**
Registrar todos los checks con su mapeo de tipos y ejecutar en cadena.

**Paso 6 â€” Scoring + ReportBuilder**
Implementar scoring y generar ambos formatos de reporte (JSON + texto).

**Paso 7 â€” CLI**
Usar `argparse` para el entry point. Retornar exit code segÃºn severidad mÃ¡xima.

### Principios de ImplementaciÃ³n

- **Nunca asumir nada del CSV:** todo se detecta o se maneja con fallback
- **Fail-safe:** cada check en try/except, error = INFO result, nunca crash total
- **Muestra de valores:** siempre incluir hasta 5 ejemplos de valores problemÃ¡ticos
- **Reproducibilidad:** el reporte debe ser determinÃ­stico dado el mismo input
- **Performance:** para DFs > 100k filas, usar muestreo estratificado en checks costosos
  (outlier detection, typo detection) con nota en el reporte
- **Sin dependencias de ML:** solo estadÃ­stica clÃ¡sica para mÃ¡xima portabilidad

### Umbrales por Defecto (Configurables)

```python
THRESHOLDS = {
    "null_rate":          {"CRITICAL": 0.50, "HIGH": 0.20, "MEDIUM": 0.05, "LOW": 0.01},
    "outlier_pct_iqr":    {"CRITICAL": 0.10, "HIGH": 0.05, "MEDIUM": 0.02, "LOW": 0.005},
    "outlier_pct_zscore": {"CRITICAL": 0.05, "HIGH": 0.02, "MEDIUM": 0.01},
    "skewness_abs":       {"HIGH": 3.0, "MEDIUM": 2.0, "LOW": 1.0},
    "duplicate_row_pct":  {"CRITICAL": 0.10, "HIGH": 0.05, "MEDIUM": 0.01},
    "rare_category_pct":  {"threshold": 0.005},   # categorÃ­as con < 0.5% de apariciÃ³n
    "id_duplicate_pct":   {"CRITICAL": 0.001},    # cualquier duplicado en ID es HIGH+
    "trend_change_std":   {"CRITICAL": 3.0, "HIGH": 2.5, "MEDIUM": 2.0},  # desviaciones
    "date_format_mix":    {"HIGH": 2, "CRITICAL": 4},  # nÂ° de formatos distintos
    "class_imbalance":    {"HIGH": 0.95, "MEDIUM": 0.90},
    "zero_value_pct":     {"HIGH": 0.30, "MEDIUM": 0.10},  # para columnas numÃ©ricas
    "levenshtein_sim":    {"threshold": 0.85},    # umbral para typo candidates
    "ks_pvalue":          {"threshold": 0.05},    # para temporal drift
}
```

---

## Patrones de DetecciÃ³n Especiales

### DetecciÃ³n de Nulls Enmascarados
Tratar como nulos los siguientes patrones en strings:
```python
NULL_LIKE = {
    '', 'null', 'none', 'nan', 'na', 'n/a', 'n.a.', '-', '--', '---',
    'missing', 'unknown', 'undefined', '?', 'nil', '#n/a', 'not available',
    'not applicable', 'sin dato', 'sin informaciÃ³n', 'desconocido'
}
# Comparar en lowercase y stripped
```

### DetecciÃ³n de Formatos de Fecha Mixtos
```python
DATE_FORMATS = [
    '%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y', '%d-%m-%Y',
    '%Y/%m/%d', '%d.%m.%Y', '%Y%m%d',
    '%Y-%m-%d %H:%M:%S', '%d/%m/%Y %H:%M:%S',
    '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M:%SZ',
    '%d %b %Y', '%B %d, %Y', '%d de %B de %Y'  # formatos en espaÃ±ol
]
```

### DetecciÃ³n de Cambio de Tendencia
Para columnas numÃ©ricas, si existe una columna de fecha:
1. Ordenar por fecha
2. Calcular media mÃ³vil de ventana = max(7, n_rows // 20)
3. Comparar cada punto con la banda [global_mean Â± N*global_std]
4. Si >5% de puntos caen fuera en el Ãºltimo 20% del perÃ­odo â†’ TREND_CHANGE alert
5. Complementar con Mann-Kendall monotonic trend test (p-value + direcciÃ³n)

---

## Ejemplo de Output Esperado

Para un CSV de ventas con columnas: `fecha`, `producto`, `monto`, `cliente_id`, `regiÃ³n`:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           DATA QUALITY AUDIT REPORT                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Archivo     : ventas_q3.csv
Filas       : 45,230
Columnas    : 5
Health Score: 61/100  (C)

RESUMEN DE ISSUES
  ğŸ”´ CRITICAL : 1
  ğŸŸ  HIGH     : 3
  ğŸŸ¡ MEDIUM   : 2
  ğŸŸ¢ LOW      : 1

PUNTOS CRÃTICOS
  [CRITICAL] monto â†’ NULL_RATE
  Detalle   : 52.3% de valores nulos en columna numÃ©rica clave
  Afectados : 23,655 registros (52.3%)
  Muestra   : [NaN, NaN, NaN, NaN, NaN]

  [HIGH] fecha â†’ DATE_FORMAT_MIX
  Detalle   : 3 formatos de fecha distintos encontrados
  Afectados : 1,204 registros (2.7%)
  Muestra   : ['2023/15/03', '15-Mar-2023', '2023-03-15']

  [HIGH] monto â†’ TREND_CHANGE
  Detalle   : Media del Ãºltimo 20% del perÃ­odo es 3.2Ïƒ por debajo del histÃ³rico
  Afectados : 9,046 registros (20.0%)
  Muestra   : [12.5, 8.3, 15.1, 9.7, 11.2]

  [HIGH] cliente_id â†’ ID_DUPLICATES
  Detalle   : 847 IDs duplicados encontrados (1.87%)
  Afectados : 847 registros (1.87%)
  Muestra   : ['C-00123', 'C-00456', 'C-00789']
```
